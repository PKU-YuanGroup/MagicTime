<h2 align="center"> <a href="https://github.com/PKU-YuanGroup">MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</a></h2>
<h5 align="center"> If you like our project, please give us a star â­ on GitHub for latest update.  </h2>


<h5 align="center">
[![arXiv](https://img.shields.io/badge/Arxiv-2401.15947-b31b1b.svg?logo=arXiv)](https://github.com/PKU-YuanGroup) 
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow)](https://github.com/PKU-YuanGroup) 
</h5>


## ğŸ“£ News
* â³â³â³ Training a stronger model with the help of Open-Sora-Plan (e.g 65 x 512 Ã— 512).
* â³â³â³ Release MagicTime and ChronoMagic dataset.
* **[2024.04.08]**  ğŸ”¥ **All codes & datasets** are coming soon! Welcome to **watch** ğŸ‘€ this repository for the latest updates.

## ğŸ˜® Highlights

MagicTIme shows excellent performance in metamorphic video generation.

### ğŸ”¥ High performance, but with fewer parameters
- with just **3B sparsely activated parameters**, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmarks.

<p align="center">
<img src="assets/intro0.jpg" width=55%>
</p>

### ğŸš€ Simple baseline, learning multi-modal interactions with sparse pathways.
- With the addition of **a simple MoE tuning stage**, we can complete the training of MoE-LLaVA on **8 A100 GPUs** within 1 days.

<p align="center">
<img src="assets/intro.jpg" width=65%>
</p>
## âš™ï¸ Requirements and Installation
We recommend the requirements as follows.

```bash
git clone https://github.com/PKU-YuanGroup/MagicTIme
```

## ğŸ—ï¸ Training & Validating
Coming soon!

## ğŸ™Œ Related Projects
* [Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA) This framework empowers the model to efficiently utilize the united visual tokens.

## ğŸ‘ Acknowledgement
* [LLaVA](https://github.com/haotian-liu/LLaVA) The codebase we built upon and it is an efficient large language and vision assistant.

## ğŸ”’ License
* The majority of this project is released under the Apache 2.0 license as found in the [LICENSE](https://github.com/PKU-YuanGroup/MoE-LLaVA/blob/main/LICENSE) file.
* The service is a research preview intended for non-commercial use only. Please contact us if you find any potential violation.



## âœï¸ Citation
If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil:.

```BibTeX
@article{lin2024moe,
  title={MoE-LLaVA: Mixture of Experts for Large Vision-Language Models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}
```




## ğŸ¤ Contributors

<a href="https://github.com/PKU-YuanGroup/MoE-LLaVA/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=PKU-YuanGroup/MoE-LLaVA" />
</a>
