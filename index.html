<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A metamorphic video generation pipeline based on the given prompts.">
  <meta name="keywords" content="Text-to-Video Generation, Diffusion Model, Time-lapse">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="color:#B9770E; font-weight: bold; font-style: italic">MagicTime</span> : Time-lapse Video Generation Models as Metamorphic Simulators</h1>
          <div class="is-size-5 publication-authors">
            <div class="author-row">
              <span class="author-block" style="font-size: 28px;">
                ✨TPAMI 2025✨
              </span>
            </div>
            <div class="author-row">
              <span class="author-block">
                <a href="https://shyuanbest.github.io/">Shenghai Yuan</a><sup>1,*</sup>,</span>
              <span class="author-block">
                <a href="https://infaaa.github.io/">Jinfa Huang</a><sup>3,*</sup>,</span>
              <span class="author-block">
                <a href="https://yujun-shi.github.io/">Yujun Shi</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://cheliosoops.github.io/YongqiXu.io/">Yongqi Xu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://ruijie-zhu.github.io/">Ruijie Zhu</a><sup>5</sup>,</span>
              <span class="author-block">
                  <a href="https://twitter.com/LinBin46984">Bin Lin</a><sup>1</sup>,</span>
            </div>
            <div class="author-row">
              <span class="author-block">
              <a href="https://cxh0519.github.io/">Xinhua Cheng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://yuanli2333.github.io/">Li Yuan</a><sup>1,2,†</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>3</sup></span>
            </div>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Peking University,</span>
            <span class="author-block"><sup>2</sup>Peng Cheng Laboratory,</span>
            <span class="author-block"><sup>3</sup>University of Rochester,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>4</sup>National University of Singapore,</span>
            <span class="author-block"><sup>5</sup>University of California Santa Cruz</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/papers/2404.05014"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.05014"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/PKU-YuanGroup/MagicTime/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/BestWishYsh/ChronoMagic"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              <!-- HuggingFace Link. -->
              <span class="link-block">
              <a href="https://huggingface.co/spaces/BestWishYsh/MagicTime"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-robot"></i>
                </span>
                <span>Demo</span>
              </a>
            </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">MagicTime</span> can generate high-quality metamorphic videos!
      </h2>
    </div>
  </div>
</section>

<style>
  .image-container img {
    width: 140px; /* Fixed width for all images */
    height: 140px; /* Fixed height for all images */
    object-fit: cover; /* Ensures the image covers the box, may clip some parts */
  }
  .text-cell {
    width: 140px; /* Allows the text cell to adjust based on content */
    text-align: center;
  }
  .square-image {
    width: 200px; /* Set both width and height to the same value */
    height: 230px;
    object-fit: cover; /* This will make sure the images cover the square area without distorting their aspect ratios */
  }
  table {
    border-collapse: collapse; /* Removes the space between borders */
  }
  table, th, td {
    border: 1.5px solid rgb(230, 230, 230); /* 给表格和单元格添加边框 */
  }
  th, td {
    text-align: center; /* 文本居中显示 */
  }
</style>

<section >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br>
        <br>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in Text-to-Video generation (T2V) have achieved remarkable success
            in synthesizing high-quality general videos from textual descriptions. A largely
            overlooked problem in T2V is that existing models have not adequately encoded physical
            knowledge of the real world, thus generated videos tend to have limited motion and poor
            variations. In this paper, we propose MagicTime, a metamorphic time-lapse video
            generation model, which learns real-world physics knowledge from time-lapse videos and
            implements metamorphic generation. First, we design a MagicAdapter scheme to decouple
            spatial and temporal training, encode more physical knowledge from metamorphic videos,
            and transform pre-trained T2V models to generate metamorphic videos. Second, we introduce
            a Dynamic Frames Extraction strategy to adapt to metamorphic time-lapse videos, which have
            a wider variation range and cover dramatic object metamorphic processes, thus embodying more
            physical knowledge than general videos. Finally, we introduce a Magic Text-Encoder to improve
            the understanding of metamorphic video prompts. Furthermore, we create a time-lapse video-text
            dataset called ChronoMagic, specifically curated to unlock the metamorphic video
            generation ability. Extensive experiments demonstrate the superiority and effectiveness of
            MagicTime for generating high-quality and dynamic metamorphic videos, suggesting time-lapse
            video generation is a promising path toward building metamorphic simulators of the physical world.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Definition</h2>
        <div class="content has-text-justified">
        <p>
          Compared to general videos, metamorphic videos contain physical knowledge, 
          long persistence, and strong variation, making them difficult to generate. We show compressed 
          .gif on github, which loses some quality. The general videos are generated by 
          the <a href="https://github.com/guoyww/AnimateDiff">Animatediff</a> and <strong>MagicTime</strong>.
        </p>
        <table style="width:100%">
          <tr>
            <td class="text-cell" style="text-align:center;"><strong>Type</strong></td>  
            <td class="text-cell" style="text-align:center;"><strong>"Bean sprouts grow and mature from seeds"</strong></td>
            <td class="text-cell" style="text-align:center;"><strong>"[...] construction in a Minecraft virtual environment"</strong></td>
            <td class="text-cell" style="text-align:center;"><strong>"Cupcakes baking in an oven [...]"</strong></td>
            <td class="text-cell" style="text-align:center;"><strong>"[...] from a tightly closed bud to [...]"</strong></td>
          </tr>
          <tr>
            <td class="text-cell" style="text-align:center;">General Videos</td>  
            <td>
              <video src="https://github.com/user-attachments/assets/d3969c02-f17d-4656-83ea-a68f1663dd1b" autoplay loop muted></video>
            </td>
            <td class="image-container" style="text-align:center;"><img src="https://github.com/user-attachments/assets/a82f3523-148c-4a84-9ff8-4cff98a4579f" alt="GeneralVideo"></td>
            <td class="image-container" style="text-align:center;"><img src="https://github.com/user-attachments/assets/ba985d8c-2337-4c8d-8ea7-58158f4545a8" alt="GeneralVideo"></td>
            <td class="image-container" style="text-align:center;"><img src="https://github.com/user-attachments/assets/04ae9b2c-578e-4461-aec0-3fb94e960b5f" alt="GeneralVideo"></td>
          </tr>
          <tr>
            <td class="text-cell" style="text-align:center;">Metamorphic Videos</td>  
            <td>
              <video src="https://github.com/user-attachments/assets/ed5360de-3a66-4ee9-a85c-cc2521aae33f" autoplay loop muted></video>
            </td>
            <td class="image-container" style="text-align:center;"><img src="https://github.com/user-attachments/assets/38d7364a-6896-432e-9ecc-71af53334dd6" alt="MetamorphicVideo"></td>
            <td class="image-container" style="text-align:center;"><img src="https://github.com/user-attachments/assets/2963146c-a502-4f12-9adb-2b50cefb91f9" alt="MetamorphicVideo"></td>
            <td class="image-container" style="text-align:center;"><img src="https://github.com/user-attachments/assets/a456badc-f1c9-45bd-be1d-915945601f76" alt="MetamorphicVideo"></td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">U-Net based Method</h2>
        <img src="https://github.com/user-attachments/assets/e00ffc85-ee83-4a22-82c0-a27a574bf1bc" alt="framework">
        <div class="content has-text-justified">
          <p>
            <b>Overview of the proposed MagicTime approach.</b> We first train MagicAdapter-S to remove the influence
            of watermarks. Next,  MagicAdapter-T is trained to generate metamorphic videos with the help of Dynamic Frames
            Extraction. Finally, we train a Magic Text-Encoder to enhance text comprehension ability. During the
            inference stage, all components need to be loaded simultaneously. Slash padding indicates the module is not used.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Gallery</h2>
        <div class="content has-text-justified">
          <p>
            We showcase some metamorphic videos generated by <strong>MagicTime</strong>, <a href="https://github.com/xuduo35/MakeLongVideo">MakeLongVideo</a>, <a href="https://github.com/modelscope">ModelScopeT2V</a>, <a href="https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file">VideoCrafter</a>, <a href="https://huggingface.co/cerspense/zeroscope_v2_576w">ZeroScope</a>, <a href="https://github.com/Vchitect/LaVie">LaVie</a>, <a href="https://github.com/Picsart-AI-Research/Text2Video-Zero">T2V-Zero</a>, <a href="https://github.com/Vchitect/Latte">Latte</a> and <a href="https://github.com/guoyww/AnimateDiff">Animatediff</a> below. Below are compressed .gif, which loses some quality.
          </p>
          <table>
            <tr>
              <td colspan="1"><center>Method</center></td>  
              <td colspan="1"><center>"cherry blossoms transitioning [...]"</center></td>
              <td colspan="1"><center>"dough balls baking process [...]"</center></td>
              <td colspan="1"><center>"an ice cube is melting [...]"</center></td>
              <td colspan="1"><center>"a simple modern house's construction [...]"</center></td>
            </tr>
            <tr>
              <td>MakeLongVideo</td>  
              <td><img src="https://github.com/user-attachments/assets/32ebcab3-8103-47e0-91cf-03325099088b" alt="MakeLongVideo"></td>
              <td><img src="https://github.com/user-attachments/assets/1b154e96-402a-4833-b4f4-c05b42fb07f4" alt="MakeLongVideo"></td>
              <td><img src="https://github.com/user-attachments/assets/cfa1468d-2703-431a-9733-49b260f00bf9" alt="MakeLongVideo"></td>
              <td><img src="https://github.com/user-attachments/assets/87f8c37c-3073-4501-ae8c-7dc0c36d519e" alt="MakeLongVideo"></td>
            </tr>
            <tr>
              <td>ModelScopeT2V</td>  
              <td><img src="https://github.com/user-attachments/assets/fadde609-2416-4e3a-8616-1a7b8b67df16" alt="ModelScopeT2V"></td>
              <td><img src="https://github.com/user-attachments/assets/e62259b1-aa16-4031-9898-abe00023dfe8" alt="ModelScopeT2V"></td>
              <td><img src="https://github.com/user-attachments/assets/f35cf24f-1c05-46ad-bae2-c6118ed721b8" alt="ModelScopeT2V"></td>
              <td><img src="https://github.com/user-attachments/assets/972584d5-1e33-41e5-85a8-a51dfb6f215b" alt="ModelScopeT2V"></td>
            </tr>
            <tr>
              <td>VideoCrafter</td>  
              <td><img src="https://github.com/user-attachments/assets/fdb200da-98c5-4698-b7b0-b856962f9fc6" alt="VideoCrafter"></td>
              <td><img src="https://github.com/user-attachments/assets/37885eb8-3f52-49fb-bfba-b62fe5350775" alt="VideoCrafter"></td>
              <td><img src="https://github.com/user-attachments/assets/a24f79df-d204-4477-b519-bad53a755b23" alt="VideoCrafter"></td>
              <td><img src="https://github.com/user-attachments/assets/5651d952-dd17-4293-8cb5-6e652053f759" alt="VideoCrafter"></td>
            </tr>
            <tr>
              <td>ZeroScope</td>  
              <td><img src="https://github.com/user-attachments/assets/b009efb6-7e45-4345-a29e-12c77c0f1df2" alt="ZeroScope"></td>
              <td><img src="https://github.com/user-attachments/assets/4afbd1d5-66ec-4f3d-b455-8068c51faff1" alt="ZeroScope"></td>
              <td><img src="https://github.com/user-attachments/assets/8df1044b-80ba-4ae6-89da-7a8b8018b89d" alt="ZeroScope"></td>
              <td><img src="https://github.com/user-attachments/assets/e024a24e-0d5c-4986-b400-3933fcdc9534" alt="ZeroScope"></td>
            </tr>
            <tr>
              <td>LaVie</td>  
              <td><img src="https://github.com/user-attachments/assets/12b9144a-1ce9-472d-98d0-86c3e1a03972" alt="LaVie"></td>
              <td><img src="https://github.com/user-attachments/assets/0fee9321-2278-45f8-a6eb-0ed3f4e11d15" alt="LaVie"></td>
              <td><img src="https://github.com/user-attachments/assets/1f4ee694-39f0-4190-8b03-d68260c1ee7e" alt="LaVie"></td>
              <td><img src="https://github.com/user-attachments/assets/3bbb43cc-5e9a-4ee1-bf51-72ee0fa67b12" alt="LaVie"></td>
            </tr>
            <tr>
              <td>T2V-Zero</td> 
              <td><img src="https://github.com/user-attachments/assets/d4e618f3-7a2f-4f17-b12d-d390013ea39c" alt="T2V-Zero"></td>
              <td><img src="https://github.com/user-attachments/assets/a02e13c3-3cef-4086-b5e9-f06218de2ef4" alt="T2V-Zero"></td>
              <td><img src="https://github.com/user-attachments/assets/55d81342-b51a-4137-a27a-878fc4d4d441" alt="T2V-Zero"></td>
              <td><img src="https://github.com/user-attachments/assets/9b6b50ba-a300-4eb6-8ce8-6d3c102acd2f" alt="T2V-Zero"></td>
            </tr>
            <tr>
              <td>Latte</td>
              <td><img src="https://github.com/user-attachments/assets/77d39bba-0486-4294-bc56-e6e3f9c1ddca" alt="Latte"></td>
              <td><img src="https://github.com/user-attachments/assets/694c6a23-af51-43c1-81eb-341964d9aa26" alt="Latte"></td>
              <td><img src="https://github.com/user-attachments/assets/dcddf819-7bd3-400e-901f-b5e4c79489a0" alt="Latte"></td>
              <td><img src="https://github.com/user-attachments/assets/069ed0a7-28db-4a34-8033-f38f3479f18c" alt="Latte"></td>
            </tr>
            <tr>
              <td>Animatediff</td>
              <td><img src="https://github.com/user-attachments/assets/13193a84-f4a4-4ba5-b49e-177c32fc410e" alt="Animatediff"></td>
              <td><img src="https://github.com/user-attachments/assets/4c961e3e-3556-497e-8695-47e1a02c0f29" alt="Animatediff"></td>
              <td><img src="https://github.com/user-attachments/assets/0e5cbd30-4e05-466e-aee7-58244ea244db" alt="Animatediff"></td>
              <td><img src="https://github.com/user-attachments/assets/2c2afd44-63e4-4ef9-9550-da89a1ebf236" alt="Animatediff"></td>
            </tr>
            <tr>
              <td>Ours</td>  
              <td><img src="https://github.com/user-attachments/assets/f6c5594a-5b49-4175-b4cf-60faa5f20db1" alt="Ours"></td>
              <td><img src="https://github.com/user-attachments/assets/4ad90c93-ba1e-4d75-8b09-294e5c291fe9" alt="Ours"></td>
              <td><img src="https://github.com/user-attachments/assets/bc6941a5-6b84-4a3b-9248-238cfade5dd4" alt="Ours"></td>
              <td><img src="https://github.com/user-attachments/assets/e7776d31-1412-40cd-bcd5-f3094310bbd8" alt="Ours"></td>
            </tr>
          </table>
          <br>
          <p>We show more metamorphic videos generated by <strong>MagicTime</strong> with the help of <a href="https://civitai.com/models/4201/realistic-vision-v20">Realistic</a>, <a href="https://civitai.com/models/30240/toonyou">ToonYou</a> and <a href="https://civitai.com/models/66347/rcnz-cartoon-3d">RcnzCartoon</a>.</p>
          <table>
            <tr>
              <td><img src="https://github.com/user-attachments/assets/8124cc26-0f27-40f3-89b2-f493400adc41" alt="Realistic"></td>
              <td><img src="https://github.com/user-attachments/assets/6ee87c95-a0cc-4948-af3a-fb70689f731a" alt="Realistic"></td>
              <td><img src="https://github.com/user-attachments/assets/1a89d55e-8a5e-45ed-a4a6-766cb16d8334" alt="Realistic"></td>
            </tr>
            <tr>
              <td colspan="1"><center>"[...] bean sprouts grow and mature from seeds"</center></td>
              <td colspan="1"><center>"dough [...] swells and browns in the oven [...]"</center></td>
              <td colspan="1"><center>"the construction [...] in Minecraft [...]"</center></td>
            </tr>
            <tr>
              <td><img src="https://github.com/user-attachments/assets/9523bc8a-b41a-477d-ba84-b7423d7fcdd8" alt="RcnzCartoon"></td>
              <td><img src="https://github.com/user-attachments/assets/68651214-3f94-4f9d-b20f-1575da7552f9" alt="RcnzCartoon"></td>
              <td><img src="https://github.com/user-attachments/assets/6555d8d5-dd03-4bf9-b4b1-113cc52d95b5" alt="RcnzCartoon"></td>
            </tr>
            <tr>
              <td colspan="1"><center>"a bud transforms into a yellow flower"</center></td>
              <td colspan="1"><center>"time-lapse of a plant germinating [...]"</center></td>
              <td colspan="1"><center>"[...] a modern house being constructed in Minecraft [...]"</center></td>
            </tr>
            <tr>
              <td><img src="https://github.com/user-attachments/assets/62168dc6-e3db-4c2e-833f-e878d7e55227" alt="ToonYou"></td>
              <td><img src="https://github.com/user-attachments/assets/f41153f3-f9fd-4836-9115-2b6a6caa741f" alt="ToonYou"></td>
              <td><img src="https://github.com/user-attachments/assets/09b74420-e4d2-48f8-81ba-ecc0ef02af57" alt="ToonYou"></td>
            </tr>
            <tr>
              <td colspan="1"><center>"an ice cube is melting"</center></td>
              <td colspan="1"><center>"bean plant sprouts grow and mature from the soil"</center></td>
              <td colspan="1"><center>"time-lapse of delicate pink plum blossoms [...]"</center></td>
            </tr>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Integrate into DiT-based Architecture</h2>
        <div class="content has-text-justified">
          <p>The mission of this project is to help reproduce Sora and provide high-quality video-text data and data annotation pipelines, to support <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">Open-Sora-Plan</a> or other DiT-based T2V models. To this end, we take an initial step to integrate our MagicTime scheme into the DiT-based Framework. Specifically, our method supports the Open-Sora-Plan v1.0.0 for fine-tuning. We first scale up with additional metamorphic landscape time-lapse videos in the same annotation framework to get the ChronoMagic-Landscape dataset. Then, we fine-tune the Open-Sora-Plan v1.0.0 with the ChronoMagic-Landscape dataset to get the MagicTime-DiT model. The results are as follows (257×512×512 (10s)):</p>
          <table>
            <tr>
              <td>
                <video src="https://github.com/user-attachments/assets/ed66b4c7-4352-456a-bee4-267090857e3e" autoplay loop muted></video>
              </td>
              <td>
                <video src="https://github.com/user-attachments/assets/f2498e6b-4556-4909-b582-954f36a71281" autoplay loop muted></video>
              </td>
              <td>
                <video src="https://github.com/user-attachments/assets/17f8d675-6315-4c48-9933-8b3a7f4e72d2" autoplay loop muted></video>
              </td>
              <td>
                <video src="https://github.com/user-attachments/assets/ec29677d-2ed7-411b-8a41-e744614916bf" autoplay loop muted></video>
              </td>
            </tr>
            <tr>
              <td><center>"Time-lapse of a coastal landscape [...]"</center></td>
              <td><center>"Display the serene beauty of twilight [...]"</center></td>
              <td><center>"Sunrise Splendor: Capture the breathtaking moment [...]"</center></td>
              <td><center>"Nightfall Elegance: Embrace the tranquil beauty [...]"</center></td>
            </tr>
            <tr>
              <td>
                <video src="https://github.com/user-attachments/assets/bc961c34-7a3d-416e-be3b-80e09a5145bc" autoplay loop muted></video>
              </td>
              <td>
                <video src="https://github.com/user-attachments/assets/4feba55e-3259-4a1c-821b-51eb905e289b" autoplay loop muted></video>
              </td>
              <td>
                <video src="https://github.com/user-attachments/assets/7a69f551-ac22-4d69-a5dd-e03819f295bb" autoplay loop muted></video>
              </td>
              <td>
                <video src="https://github.com/user-attachments/assets/b92ae91a-5f7b-4f1b-9a36-82b95478c388" autoplay loop muted></video>
              </td>
            </tr>
            <tr>
              <td colspan="1"><center>"The sun descending below the horizon [...]"</center></td>
              <td colspan="1"><center>"[...] daylight fades into the embrace of the night [...]"</center></td>
              <td colspan="1"><center>"Time-lapse of the dynamic formations of clouds [...]"</center></td>
              <td colspan="1"><center>"Capture the dynamic formations of clouds [...]"</center></td>
            </tr>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Samples of ChronoMagic dataset</h2>
          <div class="content has-text-justified">
            <p>
                In this work, we compiled a collection of time-lapse videos from the Internet to create a 
                metamorphic video-text dataset containing 2,265 videos, named the ChronoMagic. We showcases samples from the dataset below. 
                We plan to scale up the dataset to include additional categories and a larger number of videos in the future.
            </p>
            <table>
              <tr>
                <td>
                  <img src="https://github.com/user-attachments/assets/7bd715f6-c019-4863-a51e-debbd1507214" alt="Your browser does not support the video tag.">
                </td>
                <td>
                  <img src="https://github.com/user-attachments/assets/1dbb2986-0c6d-4c24-b931-7f46776e0928" alt="Your browser does not support the video tag.">
                </td>
                <td>
                  <img src="https://github.com/user-attachments/assets/10fa4806-9379-4afe-bbb1-681c6144e13e" alt="Your browser does not support the video tag.">
                </td>
              </tr>
              <tr>
                <td>
                  <img src="https://github.com/user-attachments/assets/909c0812-f175-411a-a873-00d8a753c080" alt="Your browser does not support the video tag.">
                </td>
                <td>
                  <img src="https://github.com/user-attachments/assets/5a8c8446-46b2-40de-b538-46fd4e3ce0bb" alt="Your browser does not support the video tag.">
                </td>
                <td>
                  <img src="https://github.com/user-attachments/assets/974b480a-80f0-4d18-bef3-953ab98b8f60" alt="Your browser does not support the video tag.">
                </td>
              </tr>
              <tr>
                <td>
                  <img src="https://github.com/user-attachments/assets/786a2039-123c-49de-a4e7-161e3a7d324c" alt="Your browser does not support the video tag.">
                </td>
                <td>
                  <img src="https://github.com/user-attachments/assets/9a65a2b3-9497-4709-bce3-2a11920d0b61" alt="Your browser does not support the video tag.">
                </td>
                <td>
                  <img src="https://github.com/user-attachments/assets/88882c3c-35a7-4594-be6b-0c7ae589a70e" alt="Your browser does not support the video tag.">
                </td>
              </tr>
              <!-- Repeat for other rows -->
            </table>            
          </div>
        </div>
      </div>
    </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yuan2024magictime,
  title={MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators},
  author={Yuan, Shenghai and Huang, Jinfa and Shi, Yujun and Xu, Yongqi and Zhu, Ruijie and Lin, Bin and Cheng, Xinhua and Yuan, Li and Luo, Jiebo},
  journal={arXiv preprint arXiv:2404.05014},
  year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div align=center>
      <img src="https://github.com/PKU-YuanGroup/MagicTime/blob/main/__assets__/magictime_logo.png?raw=true" width="150px">
    </div>
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2404.05014.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/PKU-YuanGroup/MagicTime" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is constructed using the templet provided by <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. Thanks for their effort.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
